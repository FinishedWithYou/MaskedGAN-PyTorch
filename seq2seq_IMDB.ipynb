{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "prog = re.compile('[A-Za-z0-9]+')\n",
    "\n",
    "path = 'aclImdb/train/unsup/'\n",
    "\n",
    "def load_imdb_data(path, seq_len=40, gen=False):\n",
    "    \"\"\"\n",
    "    Loads IMDB 50k unsupervised reviews\n",
    "    \n",
    "    path: str, path to the unsupervised reviews data\n",
    "    seq_len: minimum length of sequence\n",
    "    gen: if True all the reviews will be length of seq_len\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    \n",
    "    for i in tqdm(range(50000)):\n",
    "        with open(path + f'{i}_0.txt', 'r') as f:\n",
    "            rev = f.read()\n",
    "        \n",
    "        rev = rev.replace(' br ', ' ')\n",
    "        if len(prog.findall(rev)) >= seq_len:\n",
    "            if gen:\n",
    "                reviews.append(['<sos>'] + prog.findall(rev)[:seq_len])\n",
    "                if len(prog.findall(rev)[:seq_len]) == 39:\n",
    "                    print(len(rev.split()))\n",
    "            else:\n",
    "                reviews.append(['<sos>'] + prog.findall(rev))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 7820.06it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = load_imdb_data(path, gen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6546.22it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_40 = load_imdb_data(path, gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_idxs(data):\n",
    "    \"\"\"\n",
    "    Returns vocab, word2id and id2word, where\n",
    "    vocab: set of all words in data\n",
    "    word2id: dictionary that maps words on idxs\n",
    "    id2word: inverse dictionary to word2id\n",
    "    \n",
    "    data: \n",
    "    type: list\n",
    "    format: list of lists of words\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sentence in tqdm(data):\n",
    "        for s in sentence:\n",
    "            vocab.add(s)\n",
    "    word2id = {k:v for v, k in enumerate(vocab, 1)}\n",
    "    word2id['<m>'] = 0\n",
    "    id2word = {v:k for k, v in word2id.items()}\n",
    "    return vocab, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49668/49668 [00:01<00:00, 28692.66it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, word2id, id2word = vocab_idxs(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents2matrix(data, word2id, seq_len=41):\n",
    "    \"\"\"\n",
    "    Returns a matrix of integers\n",
    "    where each row represents a sentence\n",
    "    \n",
    "    data:\n",
    "    type: list\n",
    "    format: list of lists of words of the seq_len length\n",
    "    example: [['hello', 'world'], ['nice', 'day']]\n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    word2id: dict that maps word on idxs\n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    seq_len: len of lists contained in data\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix = np.zeros((len(data), seq_len))\n",
    "    for i in tqdm(range(len(data))):\n",
    "        matrix[i] = np.array([int(word2id[word]) for word in data[i]])\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49668/49668 [00:00<00:00, 54394.85it/s]\n"
     ]
    }
   ],
   "source": [
    "matrix = sents2matrix(reviews_40, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.LongTensor(matrix))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 110\n",
    "p = 0.5\n",
    "n_layers = 1\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedEncoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_dim, vocab_size,\n",
    "        embedding_dim, p=0.5, n_layers=1, device=\"cuda\"\n",
    "    ):\n",
    "        super(MaskedEncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.p = p\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):       \n",
    "        #generate mask\n",
    "        input = input.to(self.device)\n",
    "        mask = self.generate_mask(input.shape).long() # now masked symbols are <m> pad symbol\n",
    "        masked_input = input * mask\n",
    "        output = self.embedding(input) #.view(self.n_layers, input.shape[0], -1)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden, mask\n",
    "    \n",
    "    def generate_mask(self, size):\n",
    "        return torch.randn(size, device=self.device).ge(self.p)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_dim, n_layers=1, device=\"cuda\"):\n",
    "        super(MaskedDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MaskedDecoderRNN(hidden_dim, vocab_size, embedding_dim, device=device, n_layers=n_layers).to(device)\n",
    "encoder = MaskedEncoderRNN(hidden_dim, vocab_size, embedding_dim, device=device, p=p, n_layers=n_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)    \n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, learning_rate=0.0001, train_on_gpu=True):\n",
    "    start = time.time()\n",
    "    train_log = []\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, train_loader)\n",
    "        train_log.extend(train_loss)\n",
    "        \n",
    "        clear_output()\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch+1, n_epochs, np.mean(train_log[-100:])))\n",
    "        plot_history(train_log)\n",
    "        \n",
    "    if save_to_disk:\n",
    "        torch.save(model, 'generator.pt')\n",
    "        \n",
    "def train_epoch(encoder, decoder, encoder_optimizer, decoder_optimizer, train_loader):\n",
    "    loss_log = []\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for sequence in train_loader:\n",
    "        input = sequence[0].to(encoder.device)\n",
    "        output = input\n",
    "        loss = train(input, output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "    return loss_log\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # encoder part\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(64)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_output, encoder_hidden, mask = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    #decoder part\n",
    "    \n",
    "    decoder_input = torch.ones(input_tensor.shape[0], 1).to(decoder.device).long()\n",
    "    \n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden) # mask ubrat\n",
    "    char_column = input_tensor[:, 0].long()\n",
    "    \n",
    "    for batch_index in range(input_tensor.shape[0]):\n",
    "        if mask[batch_index, 0] == 1:\n",
    "            old_distr = torch.zeros(decoder_output[batch_index, 0].shape)\n",
    "            old_distr[input_tensor[batch_index, 0]] = 1\n",
    "            decoder_output[batch_index, 0] = old_distr\n",
    "\n",
    "    loss = criterion(decoder_output.view(input_tensor.shape[0], -1), input_tensor[:, 0])\n",
    "    \n",
    "    for char_index in range(input_tensor.shape[1] - 1):\n",
    "        decoder_input = input_tensor[:, char_index + 1].to(decoder.device).long().view(-1, 1)\n",
    "\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        char_column = input_tensor[:, char_index + 1]\n",
    "\n",
    "        for batch_index in range(input_tensor.shape[0]):\n",
    "            if mask[batch_index, char_index + 1] == 1:\n",
    "                old_distr = torch.zeros(decoder_output[batch_index, 0].shape)\n",
    "                old_distr[input_tensor[batch_index, char_index + 1]] = 1\n",
    "                decoder_output[batch_index, 0] = old_distr\n",
    "\n",
    "        loss += criterion(decoder_output.view(input_tensor.shape[0], -1), input_tensor[:, char_index + 1])\n",
    "\n",
    "#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "#     if use_teacher_forcing:\n",
    "#         # Teacher forcing: Feed the target as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "#     else:\n",
    "#         # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "#             if decoder_input.item() == EOS_token:\n",
    "#                 break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2365952548d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-baaa3d2f375b>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_epochs, learning_rate, train_on_gpu)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-baaa3d2f375b>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, train_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-baaa3d2f375b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m#                 break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation"
     ]
    }
   ],
   "source": [
    "trainIters(encoder, decoder, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
