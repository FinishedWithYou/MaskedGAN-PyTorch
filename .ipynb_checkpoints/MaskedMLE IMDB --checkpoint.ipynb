{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "prog = re.compile('[A-Za-z0-9]+')\n",
    "\n",
    "path = 'aclImdb/train/unsup/'\n",
    "\n",
    "def load_imdb_data(path, seq_len=40, gen=False):\n",
    "    \"\"\"\n",
    "    Loads IMDB 50k unsupervised reviews\n",
    "    \n",
    "    path: str, path to the unsupervised reviews data\n",
    "    seq_len: minimum length of sequence\n",
    "    gen: if True all the reviews will be length of seq_len\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    \n",
    "    for i in tqdm(range(50000)):\n",
    "        with open(path + f'{i}_0.txt', 'r') as f:\n",
    "            rev = f.read()\n",
    "        \n",
    "        rev = rev.replace(' br ', ' ')\n",
    "        if len(prog.findall(rev)) >= seq_len:\n",
    "            if gen:\n",
    "                reviews.append(['<sos>'] + prog.findall(rev)[:seq_len])\n",
    "                if len(prog.findall(rev)[:seq_len]) == 39:\n",
    "                    print(len(rev.split()))\n",
    "            else:\n",
    "                reviews.append(['<sos>'] + prog.findall(rev))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:09<00:00, 5227.41it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = load_imdb_data(path, gen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:11<00:00, 4246.86it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_40 = load_imdb_data(path, gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_idxs(data):\n",
    "    \"\"\"\n",
    "    Returns vocab, word2id and id2word, where\n",
    "    vocab: set of all words in data\n",
    "    word2id: dictionary that maps words on idxs\n",
    "    id2word: inverse dictionary to word2id\n",
    "    \n",
    "    data: \n",
    "    type: list\n",
    "    format: list of lists of words\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sentence in tqdm(data):\n",
    "        for s in sentence:\n",
    "            vocab.add(s)\n",
    "    word2id = {k:v for v, k in enumerate(vocab, 1)}\n",
    "    word2id['<m>'] = 0\n",
    "    id2word = {v:k for k, v in word2id.items()}\n",
    "    return vocab, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49668/49668 [00:01<00:00, 26335.52it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, word2id, id2word = vocab_idxs(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sents2matrix(data, word2id, seq_len=41):\n",
    "    \"\"\"\n",
    "    Returns a matrix of integers\n",
    "    where each row represents a sentence\n",
    "    \n",
    "    data:\n",
    "    type: list\n",
    "    format: list of lists of words of the seq_len length\n",
    "    example: [['hello', 'world'], ['nice', 'day']]\n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    word2id: dict that maps word on idxs\n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    seq_len: len of lists contained in data\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix = np.zeros((len(data), seq_len))\n",
    "    for i in tqdm(range(len(data))):\n",
    "        matrix[i] = np.array([int(word2id[word]) for word in data[i]])\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49668/49668 [00:00<00:00, 53029.21it/s]\n"
     ]
    }
   ],
   "source": [
    "matrix = sents2matrix(reviews_40, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.LongTensor(matrix))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PretrainedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, output_size, embedding_dim, vocab_size, n_layers=2, train_on_gpu=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.train_on_gpu = train_on_gpu\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, )\n",
    "        \n",
    "        # fully-connected layes\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, 256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(256, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(128, vocab_size))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        output, hidden = self.lstm(x)\n",
    "        output = output.view(-1, self.hidden_dim)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (self.train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "output_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = PretrainedLSTM(hidden_dim, output_size, embedding_dim, vocab_size)\n",
    "\n",
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, train_on_gpu=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    for sequence in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X = sequence[0][:, :-1]\n",
    "        y = sequence[0][:, 1:]\n",
    "        if train_on_gpu:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "        hidden = model.init_hidden(X.size(0))\n",
    "        output, hidden = model(X, hidden)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, y.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_log.append(loss.item())\n",
    "    return loss_log   \n",
    "\n",
    "def plot_history(train_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)    \n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs, train_loader, train_on_gpu=True):\n",
    "    train_log = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, train_loader)\n",
    "        train_log.extend(train_loss)\n",
    "        total_steps += len(train_loader)\n",
    "        \n",
    "        train_log.extend(train_loss)\n",
    "        \n",
    "        clear_output()\n",
    "        plot_history(train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train(model, opt, 20, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
